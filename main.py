import requests
from bs4 import BeautifulSoup
import os
import re
import time
import random
from urllib.parse import urljoin
from datetime import datetime
from openpyxl import Workbook, load_workbook
from openpyxl.drawing.image import Image as XLImage
from PIL import Image as PILImage
import argparse

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Referer': 'https://xc8866.com/',
    'Accept-Language': 'zh-CN,zh;q=0.9',
}
output_xlsx = 'output.xlsx'
base_img_dir = 'images'
crawled_file = 'crawled_posts.txt'
os.makedirs(base_img_dir, exist_ok=True)

excel_headers = ['æ ‡é¢˜', 'ä»·æ ¼', 'QQ', 'å¾®ä¿¡', 'æ‰‹æœº', 'å›¾ç‰‡1', 'å›¾ç‰‡2', 'å›¾ç‰‡3', 'å¸–å­é“¾æ¥']

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def sanitize_filename(name):
    return re.sub(r'[\\/:*?"<>|]', '_', name)

def load_crawled():
    if os.path.exists(crawled_file):
        with open(crawled_file, 'r', encoding='utf-8') as f:
            return set(line.strip().split('\t')[0] for line in f)
    return set()

def save_crawled(post_id, post_url):
    with open(crawled_file, 'a', encoding='utf-8') as f:
        f.write(post_id + '\t' + post_url + '\n')

def extract_info_from_table(soup):
    table = soup.find('table')
    price = qq = wechat = phone = ''
    if table:
        rows = table.find_all('tr')
        for row in rows:
            th = row.find('th')
            td = row.find('td')
            if not th or not td:
                continue
            label = th.get_text(strip=True)
            value = td.get_text(strip=True)
            if 'ä»·æ ¼' in label and not price:
                price = value
            elif 'QQ' in label and not qq:
                qq = value
            elif 'å¾®ä¿¡' in label and not wechat:
                wechat = value
            elif 'ç”µè¯' in label or 'æ‰‹æœº' in label:
                phone = value
    return price, qq, wechat, phone

def parse_post(post_url):
    try:
        res = requests.get(post_url, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.content, 'html.parser')

        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.has_attr('content'):
            title = meta_desc['content'].strip()
        else:
            title_tag = soup.find('h4', class_='break-all font-weight-bold ')
            title = title_tag.get_text(strip=True) if title_tag else 'æ ‡é¢˜æœªæ‰¾åˆ°'

        price, qq, wechat, phone = extract_info_from_table(soup)

        imgs_with_alt = []
        img_tags = soup.find_all('img', class_='img-fluid')
        for img_tag in img_tags:
            imgs_with_alt.append((img_tag, ''))

        return title, price, qq, wechat, phone, imgs_with_alt
    except Exception as e:
        log(f'è®¿é—®å¸–å­å¤±è´¥: {post_url} é”™è¯¯: {e}')
        return None, None, None, None, None, []

def download_images(imgs_with_alt, img_dir):
    image_files = []
    for i, (img, _) in enumerate(imgs_with_alt):
        if i >= 3:
            break
        img_url = img.get('src')
        if not img_url:
            continue
        if img_url.startswith('//'):
            img_url = 'https:' + img_url
        elif img_url.startswith('/'):
            img_url = 'https://xc8866.com' + img_url

        ext = os.path.splitext(img_url)[-1].lower()
        if not re.match(r'\.(jpg|jpeg|png|gif|bmp|webp)$', ext):
            ext = '.jpg'

        img_name = f'{i}_image{ext}'
        img_path = os.path.join(img_dir, img_name)

        try:
            resp = requests.get(img_url, headers=headers, timeout=15, stream=True)
            resp.raise_for_status()
            with open(img_path, 'wb') as f:
                for chunk in resp.iter_content(1024):
                    f.write(chunk)
            image_files.append(img_path)
            log(f'  ä¸‹è½½å›¾ç‰‡ {i+1}: {img_name}')
            time.sleep(random.uniform(0.2, 0.4))
        except Exception as e:
            log(f'å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_url}, é”™è¯¯: {e}')
            continue
    return image_files

def append_data_to_excel(rows_with_images, filename='output.xlsx'):
    if os.path.exists(filename):
        wb = load_workbook(filename)
        ws = wb.active
    else:
        wb = Workbook()
        ws = wb.active
        ws.title = "çˆ¬å–ç»“æœ"
        ws.append(excel_headers)

    for row_data in rows_with_images:
        row = row_data['row']
        imgs = row_data['images']
        ws.append(row)
        row_idx = ws.max_row
        for col_offset, img_path in enumerate(imgs[:3]):
            try:
                PILImage.open(img_path).verify()
                xl_img = XLImage(img_path)
                xl_img.width = 100
                xl_img.height = 100
                col_letter = chr(ord('F') + col_offset)
                ws.add_image(xl_img, f"{col_letter}{row_idx}")
            except Exception as e:
                log(f"âŒ å›¾ç‰‡æ’å…¥å¤±è´¥: {img_path}, é”™è¯¯: {e}")

    wb.save(filename)
    log(f"âœ… å†™å…¥ Excelï¼š{filename}")

def get_next_page_url(soup, current_url):
    a = soup.find('a', class_='page-link', string='â–¶')
    if a and a.has_attr('href'):
        href = a['href']
        full_url = urljoin(current_url, href)
        return full_url
    return None

def get_page_threads(soup):
    threads = soup.find_all('li', class_='media thread tap')
    links = [t['data-href'] for t in threads if t.has_attr('data-href')]
    return links

def crawl_pages(start_url, total_pages):
    current_url = start_url
    match = re.search(r'forum-23-(\d+)\.htm', current_url)
    if not match:
        log("âŒ èµ·å§‹é“¾æ¥æ ¼å¼ä¸æ­£ç¡®")
        return
    start_page = int(match.group(1))
    target_page = start_page + total_pages - 1
    current_page = start_page
    crawled_posts = load_crawled()
    all_page_data = []
    page_data = []

    try:
        while current_url and current_page <= target_page:
            log(f'ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µï¼š{current_url}')
            try:
                res = requests.get(current_url, headers=headers, timeout=15)
                res.raise_for_status()
                soup = BeautifulSoup(res.content, 'html.parser')

                links = get_page_threads(soup)
                if not links:
                    log(f"âš ï¸ ç¬¬ {current_page} é¡µæ²¡æœ‰è·å–åˆ°å¸–å­é“¾æ¥ï¼Œè·³è¿‡")
                    break

                log(f'ğŸ” æœ¬é¡µå…±å‘ç° {len(links)} æ¡å¸–å­é“¾æ¥')

                page_data = []  # å½“å‰é¡µæ•°æ®æ¸…ç©º

                for idx, link in enumerate(links, 1):
                    post_id = link.replace('.htm', '').replace('/', '_')
                    if post_id in crawled_posts:
                        log(f'è·³è¿‡å·²çˆ¬å–å¸–å­ {post_id} ({link})')
                        continue

                    post_url = f'https://xc8866.com/{link}'
                    log(f'â¡ï¸ æ­£åœ¨çˆ¬å–å¸–å­ {idx}/{len(links)}: {post_url}')
                    title, price, qq, wechat, phone, imgs_with_alt = parse_post(post_url)
                    if title is None:
                        log(f"âš ï¸ å¸–å­è§£æå¤±è´¥ï¼Œè·³è¿‡: {post_url}")
                        continue

                    log(f'  æ ‡é¢˜: {title}')
                    thread_img_dir = os.path.join(base_img_dir, sanitize_filename(post_id))
                    os.makedirs(thread_img_dir, exist_ok=True)

                    image_files = download_images(imgs_with_alt, thread_img_dir)
                    log(f'  ä¸‹è½½å›¾ç‰‡ {len(image_files)} å¼ ')

                    row = [title, price, qq, wechat, phone, '', '', '', post_url]
                    page_data.append({'row': row, 'images': image_files})

                    save_crawled(post_id, post_url)
                    time.sleep(random.uniform(0.8, 1.5))

                if page_data:
                    all_page_data.extend(page_data)
                    append_data_to_excel(page_data, output_xlsx)

                next_url = get_next_page_url(soup, current_url)
                if not next_url:
                    log("æ²¡æœ‰æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œç»“æŸçˆ¬å–")
                    break

                current_url = next_url
                current_page += 1

            except Exception as e:
                log(f"çˆ¬å–é¡µé¢å¤±è´¥: {current_url} é”™è¯¯: {e}")
                break

    except KeyboardInterrupt:
        log("â¸ï¸ æ£€æµ‹åˆ°æ‰‹åŠ¨ç»ˆæ­¢ (Ctrl+C)ï¼Œå¼€å§‹ä¿å­˜å½“å‰æ•°æ®...")
        combined_data = all_page_data
        if page_data:
            combined_data += page_data
        if combined_data:
            append_data_to_excel(combined_data, output_xlsx)
            log(f"âœ… å·²ä¿å­˜å½“å‰çˆ¬å–æ•°æ®åˆ°Excelï¼Œæ•°é‡ï¼š{len(combined_data)} æ¡ã€‚")
        else:
            log("âš ï¸ å½“å‰æ— æ•°æ®ï¼Œæ— éœ€ä¿å­˜ã€‚")
        log("ç¨‹åºå·²å®‰å…¨é€€å‡ºã€‚")
        exit(0)

    log("âœ… çˆ¬è™«è¿è¡Œç»“æŸ")

def main():
    parser = argparse.ArgumentParser(description="çˆ¬è™«èµ·å§‹é¡µé“¾æ¥å’Œæ€»çˆ¬å–é¡µæ•°")
    parser.add_argument('--start-url', type=str, required=True, help='èµ·å§‹é¡µé“¾æ¥ï¼Œå¦‚ https://xc8866.com/forum-23-1.htm?tagids=151_0_0_0')
    parser.add_argument('--total-pages', type=int, required=True, help='ä»èµ·å§‹é¡µå¼€å§‹ï¼Œæ€»å…±éœ€è¦çˆ¬å¤šå°‘é¡µ')
    args = parser.parse_args()

    crawl_pages(args.start_url, args.total_pages)

if __name__ == '__main__':
    main()
